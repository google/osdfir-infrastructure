## OSDFIR Infrastructure Helm Chart
## Please use this Helm chart for deploying Open Source Digital Forensics Tools
## to a Kubernetes environment.
## The following tools are supported in this deployment:
## - Timesketch; ref https://github.com/google/timesketch
## - Turbinia; ref https://github.com/google/turbinia
## - Yeti; ref https://github.com/yeti-platform/yeti
##
## @section Global parameters
## Please, note that this will override the parameters configured to use the global value
##
global:
  ## Global Persistence Configuration
  ##
  timesketch:
    ## @param global.timesketch.enabled Enables the Timesketch deployment (only used in the main OSDFIR Infrastructure Helm chart)
    ##
    enabled: true
    ## @param global.timesketch.servicePort Timesketch service port (overrides `timesketch.service.port`)
    ##
    servicePort: 5000
  turbinia:
    ## @param global.turbinia.enabled Enables the Turbinia deployment (only used within the main OSDFIR Infrastructure Helm chart)
    ##
    enabled: true
    ## @param global.turbinia.servicePort Turbinia API service port (overrides `turbinia.service.port`)
    ##
    servicePort: 8000
  dfdewey:
    ## @param global.dfdewey.enabled Enables the dfDewey deployment along with Turbinia
    ##
    enabled: false
  yeti:
    ## @param global.yeti.enabled Enables the Yeti deployment (only used in the main OSDFIR Infrastructure Helm chart)
    ##
    enabled: true
    ## @param global.yeti.servicePort Yeti API service port (overrides `yeti.api.service.port`)
    ##
    servicePort: 9000
  ## Global ingress parameters used to configure Turbinia, Timesketch, Yeti under a single loadbalancer
  ##
  ingress:
    ## @param global.ingress.enabled Enable the global loadbalancer for external access (only used in the main OSDFIR Infrastructure Helm chart)
    ##
    enabled: false
    ## @param global.ingress.className IngressClass that will be be used to implement the Ingress
    ## ref https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/
    ##
    className: "gce"
    ## @param global.ingress.selfSigned Create a TLS secret for this ingress record using self-signed certificates generated by Helm
    ##
    selfSigned: false
    ## @param global.ingress.certManager Add the corresponding annotations for cert-manager integration
    ##
    certManager: false
    ## GCP ingress configuration
    ##
    gcp:
      ## @param global.ingress.gcp.managedCertificates Enabled GCP managed certificates for your domain
      ## ref https://cloud.google.com/load-balancing/docs/ssl-certificates/google-managed-certs
      ##
      managedCertificates: false
      ## @param global.ingress.gcp.staticIPName Name of the static IP address you reserved in GCP
      ## This is required when using "gce" in ingress.className
      ## ref https://cloud.google.com/compute/docs/ip-addresses/reserve-static-external-ip-address
      ##
      staticIPName: ""
      ## @param global.ingress.gcp.staticIPV6Name Name of the static IPV6 address you reserved in GCP. This can be optionally provided to deploy a loadbalancer with an IPV6 address
      ## ref https://cloud.google.com/compute/docs/ip-addresses/reserve-static-external-ip-address
      ##
      staticIPV6Name: ""
  ## @param global.existingPVC Existing claim for the OSDFIR Infrastructure persistent volume (overrides `timesketch.persistent.name` and `turbinia.persistent.name`)
  ##
  existingPVC: osdfirvolume
  ## @param global.storageClass StorageClass for the OSDFIR Infrastructure persistent volume (overrides `timesketch.persistent.storageClass` and `turbinia.persistent.storageClass`)
  ##
  storageClass: ""
## @section OSDFIR Infrastructure persistence storage parameters
##
persistence:
  ## @param persistence.enabled Enables persistent volume storage for OSDFIR Infrastructure
  ##
  enabled: true
  ## @param persistence.name OSDFIR Infrastructure persistent volume name
  ##
  name: osdfirvolume
  ## @param persistence.size OSDFIR Infrastructure persistent volume size
  ##
  size: 2Gi
  ## @param persistence.storageClass PVC Storage Class for OSDFIR Infrastructure volume
  ## If set to "-", storageClassName: "", which disables dynamic provisioning
  ## If undefined or set to null, no storageClassName spec is
  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
  ##   GKE, AWS & OpenStack)
  ## ref https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/#using-dynamic-provisioning
  ##
  storageClass: ""
  ## @param persistence.accessModes PVC Access Mode for the OSDFIR Infrastructure volume
  ## Access mode may need to be updated based on the StorageClass
  ## ref https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes
  ##
  accessModes:
    - ReadWriteOnce
## @section Timesketch Configuration
## The following section covers configuration parameters for Timesketch
## To see a full list of available values, run helm show values charts/timesketch*
##
timesketch:
  image:
    ## @param timesketch.image.repository Timesketch image repository
    ##
    repository: us-docker.pkg.dev/osdfir-registry/timesketch/timesketch
    ## @param timesketch.image.tag Overrides the image tag whose default is the chart appVersion
    ##
    tag: latest
  frontend:
    ## Timesketch Frontend resource requests and limits
    ## ref: https://kubernetes.io/docs/user-guide/compute-resources/
    ## We leave the default resources as a choice for the user in order to increase
    ## the chances charts run on environments with little resources, such as Minikube.
    ## If you want to specify resources, uncomment the following lines, adjust them as
    ## necessary, and remove the curly braces after 'resources:'.
    ## @param timesketch.frontend.resources.limits The resources limits for the frontend container
    ## @param timesketch.frontend.resources.requests The requested resources for the frontend container
    resources:
      limits: {}
      requests: {}
  worker:
    ## Timesketch Worker resource requests and limits
    ## @param timesketch.worker.resources.limits The resources limits for the worker container
    ## @param timesketch.worker.resources.requests.cpu The requested cpu for the worker container
    ## @param timesketch.worker.resources.requests.memory The requested memory for the worker container
    ##
    resources:
      limits: {}
      requests:
        cpu: 250m
        memory: 256Mi
  config:
    ## @param timesketch.config.override Overrides the default Timesketch configs to instead use a user specified directory if present on the root directory of the Helm chart
    ## to retrieve the config files from. Please ensure the appropriate configs are in the directory else Timesketch
    ## may not run properly. To see which config files are required, review the tools/download-timesketch-configs.sh
    ## script packaged along with this Helm chart.
    ##
    override: configs/*
    ## @param timesketch.config.createUser Creates a default Timesketch user that can be used to login to Timesketch after deployment
    ##
    createUser: true
    ## Timesketch OIDC configuration
    ##
    oidc:
      ## @param timesketch.config.oidc.enabled Enables Timesketch OIDC authentication (currently only supports Google OIDC)
      ##
      enabled: false
      ## @param timesketch.config.oidc.existingSecret Existing secret with the client ID, secret and cookie secret
      ##
      existingSecret: ""
      ## Allowed emails files for Timesketch OIDC
      ##
      authenticatedEmailsFile:
        ## @param timesketch.config.oidc.authenticatedEmailsFile.enabled Enables email authentication
        ##
        enabled: true
        ## @param timesketch.config.oidc.authenticatedEmailsFile.existingSecret Existing secret with a list of emails
        ## e.g. kubectl create secret generic allowed-emails --from-file=authenticated-emails-list=allowed-emails.txt
        existingSecret: ""
        ## @param timesketch.config.oidc.authenticatedEmailsFile.content Allowed emails list (one email per line)
        ##
        content: ""
        ## One email per line
        ## e.g:
        ## content: |-
        ##   name1@domain
        ##   name2@domain
  ## Ingress Parameters
  ##
  ingress:
    ## @param timesketch.ingress.host Domain name Timesketch will be hosted under
    ## Please ensure this value is set when enabling Ingress. If using "gce" for
    ## ingress.className, please ensure you have a DNS record set for the IP address
    ## registered under ingress.gcp.staticIPName
    ##
    host: ""
  ## @section Timesketch Third Party
  ## This section contains all the main configuration for third party dependencies
  ## Timesketch requires to run
  ##
  ## @section Opensearch Configuration
  ## IMPORTANT: The Opensearch Security Plugin / TLS has not yet been configured by default
  ## ref on steps required https://opensearch.org/docs/1.1/security-plugin/configuration/index/
  ## To see a full list of available values, run helm show values charts/opensearch*
  ##
  opensearch:
    ## @param timesketch.opensearch.enabled Enables the Opensearch deployment
    ##
    enabled: true
    ## @param timesketch.opensearch.replicas Number of Opensearch instances to deploy
    ##
    replicas: 1
    persistence:
      ## @param timesketch.opensearch.persistence.size Opensearch Persistent Volume size. A persistent volume would be created for each Opensearch replica running
      ##
      size: 2Gi
    ## @param timesketch.opensearch.resources.requests Requested resources for the Opensearch containers
    ##
    resources:
      ## Example:
      ## requests:
      ##    cpu: 500m
      ##    memory: 1Gi
      requests: {}
  ## @section Redis configuration
  ## IMPORTANT: Redis is deployed with Auth enabled by default
  ## To see a full list of available values, run helm show values charts/redis*
  ##
  redis:
    ## @param timesketch.redis.enabled Enables the Redis deployment
    ##
    enabled: true
    ## @param timesketch.redis.nameOverride Overrides the Redis deployment name
    ## This is required since Turbinia and Timesketch both use Redis, we need to differentiate the instances.
    ##
    nameOverride: timesketch-redis
    ## Master Redis Service configuration
    ##
    master:
      ## @param timesketch.redis.master.count Number of Redis master instances to deploy (experimental, requires additional configuration)
      ##
      count: 1
      ## Redis master persistence configuration
      ##
      persistence:
        ## @param timesketch.redis.master.persistence.size Redis master Persistent Volume size
        ##
        size: 2Gi
      ## Redis master resource requests and limits
      ## @param timesketch.redis.master.resources.limits The resources limits for the Redis master containers
      ## @param timesketch.redis.master.resources.requests The requested resources for the Redis master containers
      resources:
        ## Example:
        ## limits:
        ##    cpu: 500m
        ##    memory: 1Gi
        limits: {}
        ## Example:
        ## requests:
        ##    cpu: 500m
        ##    memory: 1Gi
        requests: {}
    ## Redis replicas configuration parameters
    ##
    replica:
      ## @param timesketch.redis.replica.replicaCount Number of Redis replicas to deploy
      ##
      replicaCount: 0
      ## Redis replicas persistence configuration
      ##
      persistence:
        ## @param timesketch.redis.replica.persistence.size Redis replica Persistent Volume size
        ##
        size: 2Gi
      ## Redis Replica resource requests and limits
      ## @param timesketch.redis.replica.resources.limits The resources limits for the Redis replica containers
      ## @param timesketch.redis.replica.resources.requests The requested resources for the Redis replica containers
      resources:
        ## Example:
        ## limits:
        ##    cpu: 500m
        ##    memory: 1Gi
        limits: {}
        ## Example:
        ## requests:
        ##    cpu: 500m
        ##    memory: 1Gi
        requests: {}
  ## @section PostgreSQL Configuration
  ## IMPORTANT: Postgresql is deployed with Auth enabled by default
  ## To see a full list of available values, run helm show values charts/postgresql*
  ##
  postgresql:
    ## @param timesketch.postgresql.enabled Enables the Postgresql deployment
    ##
    enabled: true
    ## PostgreSQL Primary configuration parameters
    ##
    primary:
      ## PostgreSQL Primary persistence configuration
      ##
      persistence:
        ## @param timesketch.postgresql.primary.persistence.size PostgreSQL Persistent Volume size
        ##
        size: 2Gi
      ## PostgreSQL primary resource requests and limits
      ## @param timesketch.postgresql.primary.resources.limits The resources limits for the PostgreSQL primary containers
      ## @param timesketch.postgresql.primary.resources.requests The requested resources for the PostgreSQL primary containers
      resources:
        ## Example:
        ## limits:
        ##    cpu: 500m
        ##    memory: 1Gi
        limits: {}
        ## Example:
        ## requests:
        ##    cpu: 500m
        ##    memory: 1Gi
        requests: {}
    ## PostgreSQL read only replica parameters (only used when `architecture` is set to `replication`)
    ##
    readReplicas:
      ## @param timesketch.postgresql.readReplicas.replicaCount Number of PostgreSQL read only replicas
      ##
      replicaCount: 0
      ## @param timesketch.postgresql.readReplicas.persistence.size PostgreSQL Persistent Volume size
      ##
      persistence:
        size: 2Gi
      ## PostgreSQL read only resource requests and limits
      ## @param timesketch.postgresql.readReplicas.resources.limits The resources limits for the PostgreSQL read only containers
      ## @param timesketch.postgresql.readReplicas.resources.requests The requested resources for the PostgreSQL read only containers
      resources:
        ## Example:
        ## limits:
        ##    cpu: 500m
        ##    memory: 1Gi
        limits: {}
        ## Example:
        ## requests:
        ##    cpu: 500m
        ##    memory: 1Gi
        requests: {}
## @section Turbinia Configuration
## The following section covers configuration parameters for Turbinia
## To see a full list of available values, run helm show values charts/turbinia*
##
turbinia:
  ##  Turbinia server configuration
  ##
  server:
    image:
      ## @param turbinia.server.image.repository Turbinia image repository
      ##
      repository: us-docker.pkg.dev/osdfir-registry/turbinia/release/turbinia-server
      ## @param turbinia.server.image.tag Overrides the image tag whose default is the chart appVersion
      ##
      tag: latest
    ## Turbinia Server resource requests and limits
    ## @param turbinia.server.resources.limits Resource limits for the server container
    ## @param turbinia.server.resources.requests Requested resources for the server container
    resources:
      ## Example:
      ## limits:
      ##    cpu: 500m
      ##    memory: 1Gi
      limits: {}
      ## Example:
      ## requests:
      ##    cpu: 500m
      ##    memory: 1Gi
      requests: {}
  ## Turbinia Worker Configuration
  ##
  worker:
    image:
      ## @param turbinia.worker.image.repository Turbinia image repository
      ##
      repository: us-docker.pkg.dev/osdfir-registry/turbinia/release/turbinia-worker
      ## @param turbinia.worker.image.tag Overrides the image tag whose default is the chart appVersion
      ##
      tag: latest
    ## Worker autoscaler configuration
    ## ref https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
    ##
    autoscaling:
      ## @param turbinia.worker.autoscaling.enabled Enables Turbinia Worker autoscaling
      ##
      enabled: false
      ## @param turbinia.worker.autoscaling.minReplicas Minimum amount of worker pods to run at once
      ##
      minReplicas: 5
      ## @param turbinia.worker.autoscaling.maxReplicas Maximum amount of worker pods to run at once
      ##
      maxReplicas: 500
      ## @param turbinia.worker.autoscaling.targetCPUUtilizationPercentage CPU scaling metric workers will scale based on
      ##
      targetCPUUtilizationPercentage: 80
    ## Turbinia Worker resource requests and limits
    ## @param turbinia.worker.resources.limits Resources limits for the worker container
    ## @param turbinia.worker.resources.requests.cpu Requested cpu for the worker container
    ## @param turbinia.worker.resources.requests.memory Requested memory for the worker container
    ##
    resources:
      ## Example:
      ## limits:
      ##    cpu: 500m
      ##    memory: 1Gi
      limits: {}
      requests:
        cpu: 250m
        memory: 256Mi
  ## Turbinia API / Web configuration
  ##
  api:
    image:
      ## @param turbinia.api.image.repository Turbinia image repository for API / Web server
      ##
      repository: us-docker.pkg.dev/osdfir-registry/turbinia/release/turbinia-api-server
      ## @param turbinia.api.image.tag Overrides the image tag whose default is the chart appVersion
      ##
      tag: latest
    ## Turbinia API / Web resource requests and limits
    ## @param turbinia.api.resources.limits Resource limits for the api container
    ## @param turbinia.api.resources.requests Requested resources for the api container
    resources:
      ## Example:
      ## limits:
      ##    cpu: 500m
      ##    memory: 1Gi
      limits: {}
      ## Example:
      ## requests:
      ##    cpu: 500m
      ##    memory: 1Gi
      requests: {}
  ## Turbinia controller configuration
  ## The controller is not required to use Turbinia and serves as an extra
  ## container that can be used for troubleshooting
  ##
  controller:
    ## @param turbinia.controller.enabled If enabled, deploys the Turbinia controller
    ##
    enabled: false
  ## Turbinia configuration parameters
  ## ref: https://github.com/google/turbinia/blob/master/turbinia/config/turbinia_config_tmpl.py
  ##
  config:
    ## @param turbinia.config.override Overrides the default Turbinia config to instead use a user specified config. Please ensure
    ## that the config file is either placed in the root of this directory or point the override flag to a path containing
    ## your config file
    ##
    override: turbinia.conf
    ## @param turbinia.config.disabledJobs List of Turbinia Jobs to disable. Overrides DISABLED_JOBS in the Turbinia config.
    ##
    disabledJobs: "['BinaryExtractorJob', 'BulkExtractorJob', 'HindsightJob', 'PhotorecJob', 'VolatilityJob']"
    ## @param turbinia.config.existingVertexSecret Name of existing secret containing Vertex API Key in order to enable the Turbinia LLM Artifacts Analyzer. The secret must contain the key `turbinia-vertexapi`
    ## Please see https://ai.google.dev/tutorials/setup to generate your own API key.
    ## e.g. kubectl create secret generic turbinia-vertexapi-secret --from-literal=turbinia-vertexapi=VERTEX_API_KEY
    ##
    existingVertexSecret: ""
  ## GCP configuration parameters
  ## IMPORTANT: Enable GCP when running Turbinia in a GKE cluster
  ##
  gcp:
    ## @param turbinia.gcp.enabled Enables Turbinia to run within a GCP project. When enabling, please ensure you have run the supplemental script `create-gcp-sa.sh` to create a Turbinia GCP service account required for attaching persistent disks.
    ##
    enabled: false
    ## @param turbinia.gcp.projectID GCP Project ID where your cluster is deployed. Required when `.Values.gcp.enabled` is set to `true`
    ##
    projectID: ""
    ## @param turbinia.gcp.projectRegion Region where your cluster is deployed. Required when `.Values.gcp.enabled` is set to `true`
    ##
    projectRegion: ""
    ## @param turbinia.gcp.projectZone Zone where your cluster is deployed. Required when `.Values.gcp.enabled` is set to `true`
    ##
    projectZone: ""
    ## @param turbinia.gcp.gcpLogging Enables GCP Cloud Logging
    ## ref https://cloud.google.com/logging
    ##
    gcpLogging: false
    ## @param turbinia.gcp.gcpErrorReporting Enables GCP Cloud Error Reporting
    ## ref https://cloud.google.com/error-reporting/docs/grouping-errors
    ##
    gcpErrorReporting: false
  ## Turbinia service account parameters
  ##
  serviceAccount:
    ## @param turbinia.serviceAccount.create Specifies whether a service account should be created
    ##
    create: true
    ## @param turbinia.serviceAccount.annotations Annotations to add to the service account
    ##
    annotations: {}
    ## @param turbinia.serviceAccount.name The name of the Kubernetes service account to use
    ## If not set and create is true, a name is generated using the fullname template
    ##
    name: "turbinia"
    ## @param turbinia.serviceAccount.gcpName The name of the GCP service account to annotate. Applied only if `.Values.turbinia.gcp.enabled` is set to `true`
    ##
    gcpName: "turbinia"
  ## Turbinia ingress parameters
  ##
  ingress:
    ## @param turbinia.ingress.host The domain name Turbinia will be hosted under
    ## Please ensure this value is set when enabling Ingress. If using "gce" for
    ## ingress.className, please ensure you have a DNS record set for the IP address
    ## registered under ingress.gcp.staticIPName
    ##
    host: ""
  ## @section Turbinia Third Party
  ##
  ## @section Redis Configuration
  ## IMPORTANT: Redis is deployed with Auth enabled by default
  ## To see a full list of available values, run helm show values charts/redis*
  ##
  redis:
    ## @param turbinia.redis.enabled Enables the Redis deployment
    ##
    enabled: true
    ## @param turbinia.redis.nameOverride Overrides the Redis deployment name
    ## This is required since Turbinia and Timesketch both use Redis, we need to differentiate the instances.
    ##
    nameOverride: turbinia-redis
    ## Master Redis Service configuration
    ##
    master:
      ## @param turbinia.redis.master.count Number of Redis master instances to deploy (experimental, requires additional configuration)
      ##
      count: 1
      ## Redis master persistence configuration
      ##
      persistence:
        ## @param turbinia.redis.master.persistence.size Redis master Persistent Volume size
        ##
        size: 2Gi
      ## Redis master resource requests and limits
      ## @param turbinia.redis.master.resources.limits The resources limits for the Redis master containers
      ## @param turbinia.redis.master.resources.requests The requested resources for the Redis master containers
      resources:
        ## Example:
        ## limits:
        ##    cpu: 500m
        ##    memory: 1Gi
        limits: {}
        ## Example:
        ## requests:
        ##    cpu: 500m
        ##    memory: 1Gi
        requests: {}
    ## Redis replicas configuration parameters
    ##
    replica:
      ## @param turbinia.redis.replica.replicaCount Number of Redis replicas to deploy
      ##
      replicaCount: 0
      ## Redis replicas persistence configuration
      ##
      persistence:
        ## @param turbinia.redis.replica.persistence.size Redis replica Persistent Volume size
        ##
        size: 2Gi
      ## Redis Replica resource requests and limits
      ## @param turbinia.redis.replica.resources.limits The resources limits for the Redis replica containers
      ## @param turbinia.redis.replica.resources.requests The requested resources for the Redis replica containers
      resources:
        ## Example:
        ## limits:
        ##    cpu: 500m
        ##    memory: 1Gi
        limits: {}
        ## Example:
        ## requests:
        ##    cpu: 500m
        ##    memory: 1Gi
        requests: {}
  ## @section Oauth2 Proxy configuration parameters
  ## To see a full list of available values, run helm show values charts/oauth2-proxy*
  ##
  oauth2proxy:
    ## @param turbinia.oauth2proxy.enabled Enables the Oauth2 Proxy deployment
    ##
    enabled: false
    ## @param turbinia.oauth2proxy.containerPort Oauth2 Proxy container port
    ##
    containerPort: 4180
    ## OAuth2 Proxy service parameters
    ##
    service:
      ## @param turbinia.oauth2proxy.service.type OAuth2 Proxy service type
      ##
      type: ClusterIP
      ## @param turbinia.oauth2proxy.service.port OAuth2 Proxy service HTTP port
      ##
      port: 8080
    ## Configuration section
    ##
    configuration:
      ## @param turbinia.oauth2proxy.configuration.turbiniaSvcPort Turbinia service port referenced from `.Values.service.port` to be used in Oauth setup
      ##
      turbiniaSvcPort: 8000
      ## @param turbinia.oauth2proxy.configuration.existingSecret Secret with the client ID, client secret, client native id (optional) and cookie secret
      ##
      existingSecret: ""
      ## Custom configuration file: oauth2_proxy.cfg
      ## content: |
      ##   pass_basic_auth = false
      ##   pass_access_token = true
      ##
      ## @param turbinia.oauth2proxy.configuration.content [string] Oauth2 proxy configuration. Please see the [official docs](https://oauth2-proxy.github.io/oauth2-proxy/docs/configuration/overview) for a list of configurable values
      ##
      content: |
        provider="oidc"
        oidc_jwks_url="https://accounts.google.com/.well-known/openid-configuration"
        scope="https://www.googleapis.com/auth/userinfo.email"
        skip_jwt_bearer_tokens=true
        force_code_challenge_method='S256'
        logging_filename = "/tmp/oauth2proxy.log"
        standard_logging = true
        request_logging = true
        auth_logging = true
        show_debug_on_error=true
        upstreams=["http://{{ .Release.Name }}-turbinia:{{ .Values.configuration.turbiniaSvcPort }}"]
      ## Authorize individual email addresses
      ## @param turbinia.oauth2proxy.configuration.authenticatedEmailsFile.enabled Enable authenticated emails file
      ## @param turbinia.oauth2proxy.configuration.authenticatedEmailsFile.content Restricted access list (one email per line). At least one email address is required for the Oauth2 Proxy to properly work
      ## @param turbinia.oauth2proxy.configuration.authenticatedEmailsFile.existingSecret Secret with the authenticated emails file
      ##
      authenticatedEmailsFile:
        enabled: true
        ## One email per line
        ## e.g:
        ## content: |-
        ##   name1@domain
        ##   name2@domain
        ## If you override the config with restricted_access it will configure a user list within this chart what takes care of the configmap
        ##
        content: ""
        existingSecret: ""
      ## @param turbinia.oauth2proxy.configuration.oidcIssuerUrl OpenID Connect issuer URL
      ##
      oidcIssuerUrl: "https://accounts.google.com"
## @section Yeti Configuration
## The following section covers configuration parameters for Yeti
## To see a full list of available values, run helm show values charts/yeti*
##
yeti:
  ## Yeti frontend configuration
  ##
  frontend:
    image:
      ## @param yeti.frontend.image.repository Yeti frontend image repository
      ##
      repository: yetiplatform/yeti-frontend
      ## @param yeti.frontend.image.pullPolicy Yeti image pull policy
      ## ref https://kubernetes.io/docs/concepts/containers/images/#image-pull-policy
      ##
      pullPolicy: Always
      ## @param yeti.frontend.image.tag Overrides the image tag whose default is the chart appVersion
      ##
      tag: latest
    ## Yeti frontend resource requests and limits
    ## ref: https://kubernetes.io/docs/user-guide/compute-resources/
    ## @param yeti.frontend.resources.limits Resource limits for the frontend container
    ## @param yeti.frontend.resources.requests Requested resources for the frontend container
    resources:
      ## Example:
      ## limits:
      ##    cpu: 500m
      ##    memory: 1Gi
      limits: {}
      ## Example:
      ## requests:
      ##    cpu: 500m
      ##    memory: 1Gi
      requests: {}
  ## Yeti api configuration
  ##
  api:
    image:
      ## @param yeti.api.image.repository Yeti API image repository
      ##
      repository: yetiplatform/yeti
      ## @param yeti.api.image.pullPolicy Yeti image pull policy
      ## ref https://kubernetes.io/docs/concepts/containers/images/#image-pull-policy
      ##
      pullPolicy: Always
      ## @param yeti.api.image.tag Overrides the image tag whose default is the chart appVersion
      ##
      tag: latest
    ## Service Parameters
    ##
    service:
      ## @param yeti.api.service.type Yeti service type
      ##
      type: ClusterIP
      ## @param yeti.api.service.port Yeti service port
      ##
      port: 8000
    ## Yeti api resource requests and limits
    ## ref: https://kubernetes.io/docs/user-guide/compute-resources/
    ## @param yeti.api.resources.limits Resource limits for the API container
    ## @param yeti.api.resources.requests Requested resources for the API container
    resources:
      ## Example:
      ## limits:
      ##    cpu: 500m
      ##    memory: 1Gi
      limits: {}
      ## Example:
      ## requests:
      ##    cpu: 500m
      ##    memory: 1Gi
      requests: {}
  ## Yeti Tasks configuration
  ##
  tasks:
    image:
      ## @param yeti.tasks.image.repository Yeti tasks image repository
      ##
      repository: yetiplatform/yeti
      ## @param yeti.tasks.image.pullPolicy Yeti image pull policy
      ## ref https://kubernetes.io/docs/concepts/containers/images/#image-pull-policy
      ##
      pullPolicy: Always
      ## @param yeti.tasks.image.tag Overrides the image tag whose default is the chart appVersion
      ##
      tag: latest
    ## Yeti tasks resource requests and limits
    ## ref: https://kubernetes.io/docs/user-guide/compute-resources/
    ## @param yeti.tasks.resources.limits Resource limits for the tasks container
    ## @param yeti.tasks.resources.requests Requested resources for the tasks container
    resources:
      ## Example:
      ## limits:
      ##    cpu: 500m
      ##    memory: 1Gi
      limits: {}
      ## Example:
      ## requests:
      ##    cpu: 500m
      ##    memory: 1Gi
      requests: {}
  ## Ingress Parameters
  ##
  ingress:
    ## @param yeti.ingress.host Domain name Yeti will be hosted under
    ## Please ensure this value is set when enabling Ingress. If using "gce" for
    ## ingress.className, please ensure you have a DNS record set for the IP address
    ## registered under ingress.gcp.staticIPName
    ##
    host: ""
  ## @section Yeti Third Party
  ##
  ## Redis Configuration Parameters
  ##
  redis:
    ## @param yeti.redis.enabled Enables the Redis deployment
    ##
    enabled: true
    ## Master Redis Service configuration
    ##
    master:
      ## @param yeti.redis.master.count Number of Redis master instances to deploy (experimental, requires additional configuration)
      ##
      count: 1
      ## Redis master service parameters
      ##
      service:
        ## @param yeti.redis.master.service.type Redis master service type
        ##
        type: ClusterIP
        ## @param yeti.redis.master.service.ports.redis Redis master service port
        ##
        ports:
          redis: 6379
      ## Redis master persistence configuration
      ##
      persistence:
        ## @param yeti.redis.master.persistence.size Redis master Persistent Volume size
        ##
        size: 2Gi
      ## Redis master resource requests and limits
      ## @param yeti.redis.master.resources.limits The resources limits for the Redis master containers
      ## @param yeti.redis.master.resources.requests The requested resources for the Redis master containers
      resources:
        ## Example:
        ## limits:
        ##    cpu: 500m
        ##    memory: 1Gi
        limits: {}
        ## Example:
        ## requests:
        ##    cpu: 500m
        ##    memory: 1Gi
        requests: {}
  ## ArangoDB Configuration Parameters
  ##
  arangodb:
    image:
      ## @param yeti.arangodb.image.repository Yeti arangodb image repository
      ##
      repository: arangodb
      ## @param yeti.arangodb.image.pullPolicy Yeti image pull policy
      ## ref https://kubernetes.io/docs/concepts/containers/images/#image-pull-policy
      ##
      pullPolicy: Always
      ## @param yeti.arangodb.image.tag Overrides the image tag whose default is the chart appVersion
      ##
      tag: 3.11
    ## Yeti arangodb resource requests and limits
    ## ref: https://kubernetes.io/docs/user-guide/compute-resources/
    ## @param yeti.arangodb.resources.limits Resource limits for the arangodb container
    ## @param yeti.arangodb.resources.requests Requested resources for the arangodb container
    resources:
      ## Example:
      ## limits:
      ##    cpu: 500m
      ##    memory: 1Gi
      limits: {}
      ## Example:
      ## requests:
      ##    cpu: 500m
      ##    memory: 1Gi
      requests: {}
