## OpenRelik Helm Chart
## Please use this Helm chart for deploying OpenRelik to a Kubernetes environment
##
## @section Global parameters
## Please, note that this will override the parameters configured to use the global value
##
global:
  timesketch:
    ## @param global.timesketch.enabled Enables the Timesketch deployment (only used in the main OSDFIR Infrastructure Helm chart)
    ##
    enabled: false
  yeti:
    ## @param global.yeti.enabled Enables the Yeti deployment (only used in the main OSDFIR Infrastructure Helm chart)
    ##
    enabled: false
  openrelik:
    ## @param global.openrelik.enabled Enables the OpenRelik deployment (only used in the main OSDFIR Infrastructure Helm chart)
    ##
    enabled: false
  ingress:
    ## @param global.ingress.enabled Enable the global loadbalancer for external access (only used in the main OSDFIR Infrastructure Helm chart)
    ##
    enabled: false
## @section OpenRelik configuration
## The following section covers configuration parameters for OpenRelik
##
## OpenRelik frontend configuration
##
frontend:
  image:
    ## @param frontend.image.repository OpenRelik frontend image repository
    ##
    repository: ghcr.io/openrelik/openrelik-ui
    ## @param frontend.image.pullPolicy OpenRelik image pull policy
    ## ref https://kubernetes.io/docs/concepts/containers/images/#image-pull-policy
    ##
    pullPolicy: IfNotPresent
    ## @param frontend.image.tag Overrides the image tag whose default is the chart appVersion
    ##
    tag: "0.6.0"
  ## OpenRelik frontend resource requests and limits
  ## ref: https://kubernetes.io/docs/user-guide/compute-resources/
  ## @param frontend.resources.limits Resource limits for the frontend container
  ## @param frontend.resources.requests Requested resources for the frontend container
  resources:
    ## Example:
    ## limits:
    ##    cpu: 500m
    ##    memory: 1Gi
    limits: {}
    ## Example:
    ## requests:
    ##    cpu: 500m
    ##    memory: 1Gi
    requests: {}
  ## @param frontend.nodeSelector Node labels for Frontend pods assignment
  ##
  nodeSelector: {}
## @section OpenRelik API configuration
##
api:
  image:
    ## @param api.image.repository OpenRelik API image repository
    ##
    repository: ghcr.io/openrelik/openrelik-server
    ## @param api.image.pullPolicy OpenRelik image pull policy
    ## ref https://kubernetes.io/docs/concepts/containers/images/#image-pull-policy
    ##
    pullPolicy: IfNotPresent
    ## @param api.image.tag Overrides the image tag whose default is the chart appVersion
    ##
    tag: "0.6.0"
  ## OpenRelik api resource requests and limits
  ## ref: https://kubernetes.io/docs/user-guide/compute-resources/
  ## @param api.resources.limits Resource limits for the API container
  ## @param api.resources.requests Requested resources for the API container
  resources:
    ## Example:
    ## limits:
    ##    cpu: 500m
    ##    memory: 1Gi
    limits: {}
    ## Example:
    ## requests:
    ##    cpu: 500m
    ##    memory: 1Gi
    requests: {}
  ## @param api.nodeSelector Node labels for API pods assignment
  ##
  nodeSelector: {}
## @section OpenRelik Mediator configuration
##
mediator:
  image:
    ## @param mediator.image.repository OpenRelik Mediator image repository
    ##
    repository: ghcr.io/openrelik/openrelik-mediator
    ## @param mediator.image.pullPolicy OpenRelik image pull policy
    ## ref https://kubernetes.io/docs/concepts/containers/images/#image-pull-policy
    ##
    pullPolicy: IfNotPresent
    ## @param mediator.image.tag Overrides the image tag whose default is the chart appVersion
    ##
    tag: "0.6.0"
  ## OpenRelik mediator resource requests and limits
  ## ref: https://kubernetes.io/docs/user-guide/compute-resources/
  ## @param mediator.resources.limits Resource limits for the Mediator container
  ## @param mediator.resources.requests Requested resources for the Mediator container
  resources:
    ## Example:
    ## limits:
    ##    cpu: 500m
    ##    memory: 1Gi
    limits: {}
    ## Example:
    ## requests:
    ##    cpu: 500m
    ##    memory: 1Gi
    requests: {}
  ## @param mediator.nodeSelector Node labels for Mediator pods assignment
  ##
  nodeSelector: {}
## @section OpenRelik Metrics configuration
##
metrics:
  image:
    ## @param metrics.image.repository OpenRelik Metrics image repository
    ##
    repository: ghcr.io/openrelik/openrelik-metrics
    ## @param metrics.image.pullPolicy OpenRelik image pull policy
    ## ref https://kubernetes.io/docs/concepts/containers/images/#image-pull-policy
    ##
    pullPolicy: IfNotPresent
    ## @param metrics.image.tag Overrides the image tag whose default is the chart appVersion
    ##
    tag: "0.6.0"
  ## OpenRelik metrics resource requests and limits
  ## ref: https://kubernetes.io/docs/user-guide/compute-resources/
  ## @param metrics.resources.limits Resource limits for the Metrics container
  ## @param metrics.resources.requests Requested resources for the Metrics container
  resources:
    ## Example:
    ## limits:
    ##    cpu: 500m
    ##    memory: 1Gi
    limits: {}
    ## Example:
    ## requests:
    ##    cpu: 500m
    ##    memory: 1Gi
    requests: {}
  ## @param metrics.nodeSelector Node labels for Metrics pods assignment
  ##
  nodeSelector: {}
## @section OpenRelik Worker Configurations
## By default, resources are not set to allow the Helm chart to remain flexible in various deployments.
## To set resources, remove {} and replace with the provided resources
## Example
## requests:
##    cpu: 500m
##    memory: 1Gi
## limits:
##    cpu: 500m
##    memory: 1Gi
## To set environment variables, remove {} and replace with provided environment variables
## Example
## env:
##   - name: example-name
##   - value: example-value
##
workers:
- name: openrelik-worker-analyzer-config
  image: ghcr.io/openrelik/openrelik-worker-analyzer-config:0.2.0
  command: "celery --app=src.app worker --task-events --concurrency=1 --loglevel=INFO -Q openrelik-worker-analyzer-config"
  privileged: false
  replicas: 1
  env: {}
  resources: {}
  nodeSelector: {}
- name: openrelik-worker-analyzer-logs
  image: ghcr.io/openrelik/openrelik-worker-analyzer-logs:0.1.0
  command: "celery --app=src.app worker --task-events --concurrency=1 --loglevel=INFO -Q openrelik-worker-analyzer-logs"
  privileged: false
  replicas: 1
  env: {}
  resources: {}
  nodeSelector: {}
- name: openrelik-worker-bulkextractor
  image: ghcr.io/openrelik/openrelik-worker-bulkextractor:latest
  command: "celery --app=src.app worker --task-events --concurrency=1 --loglevel=INFO -Q openrelik-worker-bulkextractor"
  privileged: false
  replicas: 1
  env: {}
  resources: {}
  nodeSelector: {}
- name: openrelik-worker-capa
  image: ghcr.io/openrelik/openrelik-worker-capa:latest
  command: "celery --app=src.app worker --task-events --concurrency=1 --loglevel=INFO -Q openrelik-worker-capa"
  privileged: false
  replicas: 1
  env: {}
  resources: {}
  nodeSelector: {}
- name: openrelik-worker-chromecreds
  image: ghcr.io/openrelik/openrelik-worker-chromecreds:latest
  command: "celery --app=src.app worker --task-events --concurrency=1 --loglevel=INFO -Q openrelik-worker-chromecreds"
  privileged: false
  replicas: 1
  env: {}
  resources: {}
  nodeSelector: {}
- name: openrelik-worker-cloud-logs
  image: ghcr.io/openrelik/openrelik-worker-cloud-logs:latest
  command: "celery --app=src.app worker --task-events --concurrency=1 --loglevel=INFO -Q openrelik-worker-cloud-logs"
  privileged: false
  replicas: 1
  env: {}
  resources: {}
  nodeSelector: {}
- name: openrelik-worker-containers
  image: ghcr.io/openrelik/openrelik-worker-containers:latest
  command: "celery --app=src.app worker --task-events --concurrency=1 --loglevel=INFO -Q openrelik-worker-containers"
  privileged: false
  replicas: 1
  env: {}
  resources: {}
  nodeSelector: {}
- name: openrelik-worker-dfindexeddb
  image: ghcr.io/openrelik/openrelik-worker-dfindexeddb:0.1.0
  command: "celery --app=src.app worker --task-events --concurrency=1 --loglevel=INFO -Q openrelik-worker-dfindexeddb"
  privileged: false
  replicas: 1
  env: {}
  resources: {}
  nodeSelector: {}
- name: openrelik-worker-extraction
  image: ghcr.io/openrelik/openrelik-worker-extraction:0.4.0
  command: "celery --app=src.app worker --task-events --concurrency=1 --loglevel=INFO -Q openrelik-worker-extraction"
  privileged: false
  replicas: 1
  env: {}
  resources: {}
  nodeSelector: {}
- name: openrelik-worker-floss
  image: ghcr.io/openrelik/openrelik-worker-floss:latest
  command: "celery --app=src.app worker --task-events --concurrency=1 --loglevel=INFO -Q openrelik-worker-floss"
  privileged: false
  replicas: 1
  env: {}
  resources: {}
  nodeSelector: {}
- name: openrelik-worker-grep
  image: ghcr.io/openrelik/openrelik-worker-grep:0.1.0
  command: "celery --app=src.app worker --task-events --concurrency=1 --loglevel=INFO -Q openrelik-worker-grep"
  privileged: false
  replicas: 1
  env: {}
  resources: {}
  nodeSelector: {}
- name: openrelik-worker-hayabusa
  image: ghcr.io/openrelik/openrelik-worker-hayabusa:0.3.0
  command: "celery --app=src.app worker --task-events --concurrency=1 --loglevel=INFO -Q openrelik-worker-hayabusa"
  privileged: false
  replicas: 1
  env: {}
  resources: {}
  nodeSelector: {}
- name: openrelik-worker-plaso
  image: ghcr.io/openrelik/openrelik-worker-plaso:0.4.0
  command: "celery --app=src.app worker --task-events --concurrency=1 --loglevel=INFO -Q openrelik-worker-plaso"
  privileged: false
  replicas: 1
  env: {}
  resources: {}
  nodeSelector: {}
- name: openrelik-worker-os-creds
  image: ghcr.io/openrelik/openrelik-worker-os-creds:latest
  command: "celery --app=src.app worker --task-events --concurrency=1 --loglevel=INFO -Q openrelik-worker-os-creds"
  privileged: false
  replicas: 1
  env: {}
  resources: {}
  nodeSelector: {}
- name: openrelik-worker-strings
  image: ghcr.io/openrelik/openrelik-worker-strings:0.2.1
  command: celery --app=src.app worker --task-events --concurrency=1 --loglevel=INFO -Q openrelik-worker-strings
  privileged: false
  replicas: 1
  env: {}
  resources: {}
  nodeSelector: {}
## When deployed via the OSDFIR Infrastructure chart, the Timesketch Worker automatically receives its required environment
## variables through Helm.
##
- name: openrelik-worker-timesketch
  image: ghcr.io/openrelik/openrelik-worker-timesketch:0.3.0
  command: "celery --app=src.app worker --task-events --concurrency=1 --loglevel=INFO -Q openrelik-worker-timesketch"
  privileged: false
  replicas: 1
  env: {}
  resources: {}
  nodeSelector: {}
- name: openrelik-worker-yara
  image: ghcr.io/openrelik/openrelik-worker-yara:latest
  command: "celery --app=src.app worker --task-events --concurrency=1 --loglevel=INFO -Q openrelik-worker-yara"
  privileged: false
  replicas: 1
  env: {}
  resources: {}
  nodeSelector: {}
## @section OpenRelik Configuration Parameters
##
config:
  ## @param config.existingSecret Use an existing secret as the OpenRelik secret.
  ## Secret name must follow the naming convention of {{ .Release.Name }}-openrelik-secret
  ##
  existingSecret: false
  ## @param config.gkeAllowList Specify a GKE Autopilot allowlist for OpenRelik privileged workers to use
  ##
  gkeAllowList: ""
  ## OpenRelik nbd init container configuration for OpenRelik containers that require the nbd module.
  ## Only enable this container when you need to load the nbd kernel module to the underlying node.
  ##
  initWorkerNbd:
    ## @param config.initWorkerNbd.enabled Whether to enable the nbd init container
    ##
    enabled: false
    ## @param config.initWorkerNbd.image Container image with modprobe installed necessary
    ## for loading the nbd module
    ##
    image: "us-central1-docker.pkg.dev/osdfir-registry/openrelik/openrelik-nbd-init:0.1.0"
  ## @param config.createUser Create a default `openrelik` user.
  ##
  createUser: true
  ## OpenRelik OIDC configuration
  ##
  oidc:
    ## @param config.oidc.enabled Enables OpenRelik OIDC authentication (currently only supports Google OIDC)
    ##
    enabled: false
    ## @param config.oidc.existingSecret Existing secret with the client ID, secret and cookie secret
    ##
    existingSecret: ""
    ## @param config.oidc.workspaceDomain Restricts logins from a Google Workspace domain.
    ##
    workspaceDomain: ""
    ## Allowed emails files for OpenRelik OIDC
    ##
    authenticatedEmailsFile:
      ## @param config.oidc.authenticatedEmailsFile.enabled Enables email authentication
      ##
      enabled: false
      ## @param config.oidc.authenticatedEmailsFile.existingSecret Existing secret with a list of emails
      ## e.g. kubectl create secret generic allowed-emails --from-file=authenticated-emails-list=allowed-emails.txt
      existingSecret: ""
      ## @param config.oidc.authenticatedEmailsFile.content Allowed emails list (one email per line)
      ##
      content: ""
      ## One email per line
      ## e.g:
      ## content: |-
      ##   name1@domain
      ##   name2@domain
## Persistence Storage Parameters
##
persistence:
  ## @param persistence.existingPVC Specify an existing PVC to use for the OpenRelik volume
  ##
  existingPVC: ""
  ## @param persistence.size OpenRelik persistent volume size
  ##
  size: 2Gi
  ## @param persistence.storageClass PVC Storage Class for OpenRelik volume
  ## If set to "-", storageClassName: "", which disables dynamic provisioning
  ## If undefined or set to null, no storageClassName spec is
  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
  ##   GKE, AWS & OpenStack)
  ## ref https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/#using-dynamic-provisioning
  ##
  storageClass: ""
  ## @param persistence.accessModes PVC Access Mode for OpenRelik volume
  ## Access mode may need to be updated based on the StorageClass
  ## ref https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes
  ##
  accessModes:
    - ReadWriteOnce
## @section Redis Configuration Parameters
##
redis:
  ## Redis image configuration
  ##
  image:
    ## @param redis.image.repository Redis image repository
    ##
    repository: redis
    ## @param redis.image.tag Redis image tag
    ##
    tag: "7.4.2-alpine"
  ## Redis persistence configuration
  ##
  persistence:
    ## @param redis.persistence.size Redis Persistent Volume size
    ##
    size: 2Gi
  ## Redis resource requests and limits
  ## @param redis.resources.limits The resources limits for the Redis containers
  ## @param redis.resources.requests The requested resources for the Redis containers
  resources:
    ## Example:
    ## limits:
    ##    cpu: 500m
    ##    memory: 1Gi
    limits: {}
    ## Example:
    ## requests:
    ##    cpu: 500m
    ##    memory: 1Gi
    requests: {}
  ## @param redis.nodeSelector Node labels for Redis pods assignment
  ##
  nodeSelector: {}
## @section Postgresql Configuration Parameters
##
postgresql:
  ## Postgresql image configuration
  ##
  image:
    ## @param postgresql.image.repository Postgresql image repository
    ##
    repository: postgres
    ## @param postgresql.image.tag Postgresql image tag
    ##
    tag: "17.5-alpine"
  ## PostgreSQL persistence configuration
  ##
  persistence:
    ## @param postgresql.persistence.size PostgreSQL Persistent Volume size
    ##
    size: 2Gi
  ## PostgreSQL resource requests and limits
  ## @param postgresql.resources.limits The resources limits for the PostgreSQL primary containers
  ## @param postgresql.resources.requests.cpu The requested cpu for the PostgreSQL primary containers
  ## @param postgresql.resources.requests.memory The requested memory for the PostgreSQL primary containers
  ##
  resources:
    ## Example:
    ## limits:
    ##    cpu: 500m
    ##    memory: 1Gi
    limits: {}
    ## Example:
    ## requests:
    ##    cpu: 500m
    ##    memory: 1Gi
    requests:
      cpu: 250m
      memory: 256Mi
  ## @param postgresql.nodeSelector Node labels for Postgresql pods assignment
  ##
  nodeSelector: {}
## @section Prometheus Configuration Parameters
##
prometheus:
  ## Prometheus image configuration
  ##
  image:
    ## @param prometheus.image.repository Prometheus image repository
    ##
    repository: "prom/prometheus"
    ## @param prometheus.image.tag Prometheus image tag
    ##
    tag: "v3.0.1"
  ## Prometheus persistence configuration
  ##
  persistence:
    ## @param prometheus.persistence.size Prometheus Persistent Volume size
    ##
    size: 2Gi
  ## Prometheus resource requests and limits
  ## @param prometheus.resources.limits The resources limits for the Prometheus containers
  ## @param prometheus.resources.requests The requested resources for the Prometheus containers
  resources:
    ## Example:
    ## limits:
    ##    cpu: 500m
    ##    memory: 1Gi
    limits: {}
    ## Example:
    ## requests:
    ##    cpu: 500m
    ##    memory: 1Gi
    requests: {}
  ## @param prometheus.nodeSelector Node labels for Prometheus pods assignment
  ##
  nodeSelector: {}
